# Feature Specification: Features Test GAP Closure

**Feature Branch**: `005-features-test-gap`  
**Created**: 2025-11-01  
**Status**: Draft  
**Input**: User description: "Close the GAP in the features-test SPEC"

## Clarifications

### Session 2025-11-01

- Q: Should global scenarios run when specific features are selected? → A: Only when no specific features are provided (default run). When features are explicitly selected with `--features`, do not run global scenarios unless `--global-scenarios-only` is set.
- Q: What is the exact JSON output shape and content when `--json` is used? → A: Strict array of objects: `[ { "testName": string, "result": boolean } ]` with no additional fields, envelopes, or NDJSON; counts/aggregates are left to the consumer.
- Q: What is the exit behavior when zero tests are selected or discovered post-filtering? → A: Exit 0 with a clear "no tests found" summary; in `--json` mode output exactly `[]`.
- Q: Should duplicate/idempotence tests run during `--global-scenarios-only`? → A: No. Duplicate/idempotence tests are per-feature and SHOULD NOT run in global-only mode.
- Q: What are the filtering semantics for `--filter` on scenario names? → A: Case-sensitive substring match on scenario names only (not applied to feature IDs or filenames).

## User Scenarios & Testing (mandatory)

### User Story 1 - Run a complete features test suite (Priority: P1)

As a feature author, I can run a complete test suite for my features collection and receive a clear, per-test summary so I know exactly which autogenerated tests, scenarios, and duplicate/idempotence checks passed or failed.

**Why this priority**: This is the core value of the subcommand—ensuring feature quality and preventing regressions before packaging/publishing.

**Independent Test**: Provide a valid collection with `src/` and `test/`. Run the test command on the collection. Verify that autogenerated tests, scenarios, and duplicate checks execute; include `_global` scenarios only when no specific features are selected; results are aggregated, a summary is shown, and the exit code reflects overall pass/fail.

**Acceptance Scenarios**:

1. Given a collection with `src/<feature-id>/` and `test/<feature-id>/test.sh`, when I run the test command with default options, then the autogenerated test runs for each discovered feature and each result is listed in the final summary.
2. Given scenario definitions and scripts for a feature, when I run the test command, then each matching scenario executes (respecting any filter) and each result is listed with the scenario name.
3. Given duplicate/idempotence checks are enabled by default, when the test suite runs, then a duplicate install test is executed per feature and its result appears in the summary.
4. Given any test fails, when the command finishes, then the exit code is non-zero and the summary identifies failing tests.
5. Given no `--features` are provided, when I run, then `_global` scenarios under `test/_global` are included; given `--features` are provided, `_global` scenarios are excluded unless `--global-scenarios-only` is set.

---

### User Story 2 - Control scope and behavior via flags (Priority: P1)

As a CI engineer, I can precisely select which features and scenarios to run and control output and cleanup, so that pipeline runs are deterministic, readable, and leave no residue unless I opt in.

**Why this priority**: CI needs reliable, selective execution and clean environments to reduce flakiness and cost.

**Independent Test**: Exercise CLI with `--project-folder`, `--features`, `--filter`, `--skip-*`, `--global-scenarios-only`, `--base-image`, `--remote-user`, `--preserve-test-containers`, `--quiet`, and invalid combinations to verify behavior and validations.

**Acceptance Scenarios**:

1. Given `--features a b`, when I run, then only `a` and `b` tests execute and others are skipped.
2. Given `--filter "basic"`, when I run, then only scenarios whose names include the case-sensitive substring "basic" execute (filter applies to scenario names only).
3. Given `--global-scenarios-only`, when I run, then only tests under `test/_global` execute and no per-feature tests run.
4. Given `--skip-scenarios`, when I run, then no scenario tests execute; invalid combos (e.g., with `--filter`) produce immediate argument errors before any tests start.
5. Given I run without `--preserve-test-containers`, when execution completes, then no test-labeled containers remain; with `--preserve-test-containers`, they remain and are discoverable by label.
6. Given my selections/filters result in zero tests (e.g., no matching features or scenarios), when I run, then the command exits 0 with a clear "no tests found" summary; with `--json`, stdout is exactly `[]`.
7. Given `--global-scenarios-only` is set, when I run, then duplicate/idempotence tests do not execute (even if `--skip-duplicated` is not provided) and the summary counts exclude duplicate checks.

---

### User Story 3 - Clear failures and validations (Priority: P2)

As a maintainer, when preconditions are not met (missing folders, missing test scripts, malformed scenario definitions, or unavailable container runtime), I receive clear, actionable errors and a non-zero exit without any silent fallback.

**Why this priority**: Fast failure with clear messages reduces triage time and prevents false confidence.

**Independent Test**: Intentionally run with missing `src/` or `test/`, missing `test.sh`, malformed scenario JSONC, invalid flag combinations, or without a container runtime to confirm validation and error reporting.

**Acceptance Scenarios**:

1. Given the project folder lacks `src/` or `test/`, when I run, then the command exits non-zero and prints a clear message naming the missing folder(s).
2. Given `test/<feature>/test.sh` is missing while autogenerated tests are enabled, when I run, then execution fails with a message pointing to the missing script.
3. Given malformed `scenarios.json`, when I run, then parsing errors are listed and the command exits non-zero.
4. Given the container runtime is unavailable, when I run, then the command exits non-zero with a clear message; no silent success is reported.

---

### Edge Cases

- Empty `test/` or no matching features after selection/filtering.
- Scenario name collisions or names with special characters.
- Network or environment interruptions during environment preparation or image acquisition.
- Scripts exiting with non-zero codes; ensure results and exit code propagate.
- Cross-platform path normalization for scenario/test discovery.
- Paths with spaces or unusual characters in collection roots or feature IDs.

## Requirements (mandatory)

### Functional Requirements

- **FR-001 (CLI flags)**: The command recognizes and validates all flags and options described in the Features Test specification, including: `--project-folder (-p)`, `--features (-f)`, `--filter`, `--global-scenarios-only`, `--skip-scenarios`, `--skip-autogenerated`, `--skip-duplicated`, `--permit-randomization`, `--base-image (-i)`, `--remote-user (-u)`, `--log-level`, `--preserve-test-containers`, and `--quiet (-q)`. Invalid combinations error out before execution.
- **FR-002 (Structure validation)**: The provided project folder MUST contain `src/` and `test/`. Missing folders cause an immediate, clear error and exit non-zero.
- **FR-003 (Autogenerated tests)**: For each selected feature with `test/<feature-id>/test.sh`, execute the autogenerated test and collect a pass/fail result labeled by feature.
- **FR-004 (Scenario tests)**: Discover scenarios for each selected feature (and optional `_global`) via scenario definitions; execute matching scenario scripts, support substring filtering, and record per-scenario results.
	- Scenario definition location and schema: Scenario definitions MUST be discovered at `test/<feature>/scenarios.json` or `test/<feature>/scenarios.jsonc` (JSONC supported). Each scenario object MUST include `name: string`. Additional fields (e.g., `script: string`, `args: string[]`) are implementation-defined.
- **FR-005 (Duplicate/idempotence)**: When enabled (default), run a duplicate install/idempotence check per feature and record its result.
- **FR-005a (Global-only duplicate policy)**: In `--global-scenarios-only` mode, do not run duplicate/idempotence tests (duplicates apply only to per-feature runs).
- **FR-006 (Selection and filtering)**: `--features` restricts to specified feature IDs; `--filter` restricts scenarios by substring; `--global-scenarios-only` and `--features` are mutually exclusive; `--skip-scenarios` cannot be combined with `--global-scenarios-only` or `--filter`.
- **FR-006a (Filter semantics)**: `--filter` applies a case-sensitive substring match on scenario names only (does not apply to feature IDs, paths, or script filenames).
- **FR-007 (Environment defaults)**: Autogenerated tests use a default base environment (default image reference per spec: "ubuntu:focal") and honor an optional remote user when provided. When `--base-image` is not provided, the default base image MUST be `ubuntu:focal`.
- **FR-008 (Lifecycle and cleanup)**: Test environments are tagged/labeled for identification. By default all test environments are removed after execution; when `--preserve-test-containers` is set, they remain and are discoverable by label.
- **FR-009 (Output)**: Produce readable text output with section headings, per-test lines (including feature and scenario names), and a final summary indicating totals and pass/fail status; `--quiet` reduces verbosity but still surfaces failures and the final summary.
- **FR-010 (Exit behavior)**: Exit code is 0 only when all executed tests pass; exit code is non-zero for any failure or validation error. Runtime unavailability MUST result in a non-zero exit and a clear, user-facing error (no silent fallbacks).
- **FR-010a (No-tests behavior)**: If zero tests are selected/discovered after applying flags and filters, exit 0 and print a clear "no tests found" message; in `--json` mode, output exactly an empty array `[]`.
	- Examples:
		- Text mode:
			```text
			No tests found (after selection/filter).
			```
		- JSON mode:
			```json
			[]
			```
- **FR-011 (Logging level)**: Honor `--log-level` to adjust verbosity of diagnostic messages while keeping user-facing results readable.
- **FR-012 (Structured results)**: Provide a per-test results list suitable for consumption and aggregation. Include a JSON output mode enabled by `--json` that outputs only a JSON array of objects with shape `[ { "testName": string, "result": boolean } ]`. By default (no flag), output is a human-readable text summary.
- **FR-012a (JSON mode strictness)**: In `--json` mode, output exactly a single JSON array as specified in FR-012 with no additional fields (e.g., no `failureReason`, `summary`, or top-level envelope) and not as NDJSON.
- **FR-013 (Cross-platform behavior)**: Behave consistently across supported hosts (Linux, macOS, Windows/WSL) for path handling and discovery semantics. Discovery and filtering are case-sensitive; path separators MUST be normalized (e.g., treat `\\` and `/` equivalently); spaces and unusual characters in paths and feature IDs MUST be supported.
- **FR-014 (Global scenarios inclusion)**: Global scenarios are executed automatically only when no specific features are selected; when `--features` is provided, global scenarios are excluded unless `--global-scenarios-only` is set.

#### Additional Flag Status

- **FR-001a (Randomization flag status)**: The `--permit-randomization` flag is Deferred as of this spec version. If provided, the CLI MUST emit a clear, user-facing error (e.g., `Not implemented yet: randomization`) and abort without executing tests (see Constitution III: No Silent Fallbacks).

### Key Entities

- **Test Collection**: A folder with `src/` and `test/` defining features and their tests.
- **Feature**: A testable unit identified by `<feature-id>` with corresponding `src/<feature-id>/` and `test/<feature-id>/`.
- **Scenario**: A named test configuration and script associated with a feature (or `_global`) that exercises specific behaviors.
- **Test Result**: A record containing a test name (feature, feature + scenario, or duplicate) and a boolean pass/fail outcome.
- **Test Run**: An execution session consisting of selected features/scenarios, configuration flags, and an aggregated summary.

## Success Criteria (mandatory)

### Measurable Outcomes

- **SC-001 (Discovery and execution)**: Given a valid collection, the command discovers all targeted autogenerated tests and scenarios and executes them, producing a list of per-test results and a final summary for 100% of discovered items.
- **SC-002 (Selection fidelity)**: With `--features` and/or `--filter`, only the specified features and scenarios execute (0 unintended items) while totals and summary reflect the filtered set.
- **SC-003 (Validation and errors)**: Invalid flag combinations, missing folders, missing `test.sh`, malformed scenario definitions, or unavailable runtime produce clear error messages and a non-zero exit before or after execution as appropriate.
- **SC-004 (Cleanup)**: By default, no test-labeled environments remain after the run; with `--preserve-test-containers`, test-labeled environments remain and are enumerable by label.
- **SC-005 (User experience)**: In quiet mode, informational noise is reduced while failures and the final summary remain visible; users can determine overall success/failure within a single screen of output for suites up to dozens of tests.
- **SC-006 (Timeliness)**: For a single feature with one autogenerated test and one scenario, a typical run completes within a few minutes on a standard developer machine; long waits are limited to environment provisioning when required.
- **SC-007 (Zero tests)**: When zero tests are discovered post-selection, the command exits 0; text mode shows "no tests found"; `--json` prints `[]`.

### Output Mode Verification

- Given `--json` is provided, then stdout is exactly a JSON array of `{ testName, result }` objects with no extra keys; aggregates are omitted.

### Assumptions

- Collections follow the standard `src/` and `test/` layout described in the specification.
- Tests execute in isolated environments and do not depend on undeclared host state.
- The default base environment for autogenerated tests satisfies typical POSIX shell expectations.



